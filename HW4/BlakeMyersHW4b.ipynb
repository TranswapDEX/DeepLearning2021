{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of HW4b",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "(Notebook modified from https://course.ccs.neu.edu/ds4440f20/)\n",
        "\n",
        "# LSTMs and sequence2 sequence models\n",
        "\n",
        "**Instructions:** Answer the questions below in the notebook itself. Submit on canvas your notebook and a pdf printout of your notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EPLQRFFssrp"
      },
      "source": [
        "## Let's work through our exercise: learning to add (with strings)\n",
        "\n",
        "This idea borrowed from the official Keras docs -- I have borrowed some of their data generation code but written PyTorch version. See original version (keras) here: https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py\n",
        "\n",
        "Additional useful links (torch): https://github.com/bentrevett/pytorch-seq2seq\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIh-YgNUCTTU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsCnTPqRxB6L"
      },
      "source": [
        "class CharacterTable(object):\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "    def __init__(self, chars, one_hot=False):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "        self.one_hot = one_hot\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        if self.one_hot:\n",
        "          x = np.zeros((num_rows, len(self.chars)))\n",
        "          for i, c in enumerate(C):\n",
        "              x[i, self.char_indices[c]] = 1\n",
        "        else:\n",
        "          x = np.zeros(num_rows)\n",
        "          for i, c in enumerate(C):\n",
        "            x[i] = self.char_indices[c]\n",
        "          \n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghp5MKg87u-Z"
      },
      "source": [
        "Test the encoding and decoding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbXwn8gg7ERQ",
        "outputId": "91aed4c8-82be-49c2-d59b-6a0e4339f2c2"
      },
      "source": [
        "import random\n",
        "vocabulary = ['a', 'b', 'c', 'd', 'e']\n",
        "\n",
        "ct = CharacterTable(vocabulary, True)\n",
        "\n",
        "seq = ''.join([random.choice(vocabulary) for i in range(5)])\n",
        "seq_enc = ct.encode(seq, 10)\n",
        "print(seq)\n",
        "print(seq_enc)\n",
        "print(ct.decode(seq_enc))\n",
        "assert seq == ct.decode(seq_enc)[:len(seq)]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "ct = CharacterTable(vocabulary, False)\n",
        "seq_enc = ct.encode(seq, 10)\n",
        "print(seq)\n",
        "print(seq_enc)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bddba\n",
            "[[0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "bddbaaaaaa\n",
            "================================================================================\n",
            "bddba\n",
            "[1. 3. 3. 1. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9UYEVjGxs_i"
      },
      "source": [
        "Next generate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8jb8dmwxusU",
        "outputId": "ee4f4f66-753d-48b6-8a0d-d1a54fd6371f"
      },
      "source": [
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = '0123456789+ '\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions, expected = [], []\n",
        "seen = set()\n",
        "\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
        "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    \n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = '{}+{}'.format(a, b)\n",
        "    query = q + ' ' * (MAXLEN - len(q))\n",
        "    questions.append(query)\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
        "    expected.append(ans)\n",
        "print('Total addition questions:', len(questions))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total addition questions: 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBjVaZyXChjX",
        "outputId": "9282f794-dc72-441e-dfdc-4577b51dc2a0"
      },
      "source": [
        "# e.g.\n",
        "print(list(zip(questions[:5], expected[:5])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('85+4   ', '89  '), ('153+1  ', '154 '), ('13+37  ', '50  '), ('166+7  ', '173 '), ('9+49   ', '58  ')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBmry7Fxybag"
      },
      "source": [
        "Now vectorize, split into train and val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsHm9_TAybl6",
        "outputId": "9a84c259-2e9c-495a-dd91-b5ba414b7a29"
      },
      "source": [
        "print('Vectorization...')\n",
        "# note that the pytorch (nn) Embedding layer wants indices as inputs (not one-hot)\n",
        "x = np.zeros((len(questions), MAXLEN), dtype=int) # len(chars)), dtype=int)\n",
        "y = np.zeros((len(questions), DIGITS + 1), dtype=int) #, len(chars)), dtype=int)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "    \n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "x[0].shape # MAXLEN x num chars"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYw9xyCojAR3",
        "outputId": "758e8d97-5509-4cf0-a779-5f1439c02e2d"
      },
      "source": [
        "x[0,:]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10,  7,  1,  6,  0,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bSPok_t7w1_",
        "outputId": "09b11c8a-4c36-4fe8-e58e-1e4872ae0485"
      },
      "source": [
        "y[0].shape # (DIGITS + 1) x num chars"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALnBQn1yFoSn",
        "outputId": "959bbe91-da1d-4b7b-879c-2ba2e3e03361"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10, 11,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAOeZgapyzLA"
      },
      "source": [
        "Shuffle, split into train/val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r93RRiX6yg3P",
        "outputId": "0f9c6921-29bc-4e7e-c72e-9d4e8264e125"
      },
      "source": [
        "# Shuffle (x, y) in unison\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = torch.from_numpy(x[indices])\n",
        "y = torch.from_numpy(y[indices])\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print('Training Data:')\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print()\n",
        "print('Validation Data:')\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data:\n",
            "torch.Size([45000, 7])\n",
            "torch.Size([45000, 4])\n",
            "\n",
            "Validation Data:\n",
            "torch.Size([5000, 7])\n",
            "torch.Size([5000, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA7Bhp-t8r3w"
      },
      "source": [
        "Now let's define our encoder/decoder model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMRjjZVODUmo",
        "outputId": "1431bfc4-f3ea-4a86-d97d-e2d86cb89f8c"
      },
      "source": [
        "x[0,:]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10,  4,  8,  1,  9,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohdmnc8p9So_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_dim, emb_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input_dim = input_dim\n",
        "    self.embed_dim = emb_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    \n",
        "    self.embed_layer = nn.Embedding(self.input_dim, self.embed_dim)\n",
        "    # batch_first --> (batch, seq, feature)\n",
        "    self.rnn = nn.LSTM(self.embed_dim, self.hidden_dim, batch_first=True)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # x_e = (batch x length x dims)\n",
        "    x_e = self.embed_layer(x)\n",
        "    outputs, h = self.rnn(x_e)\n",
        "    return h\n",
        "        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLJYxnI6U1PB"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_dim, embed_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed_dim = embed_dim\n",
        "    self.output_dim = output_dim\n",
        "    \n",
        "    self.embed_layer = nn.Embedding(self.input_dim, self.embed_dim)\n",
        "    # batch_first --> (batch, seq, feature)\n",
        "    self.rnn = nn.LSTM(self.embed_dim, self.hidden_dim, batch_first=True)\n",
        "    self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "    self.sm = nn.Softmax(dim=-1)\n",
        "    \n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    x_e = self.embed_layer(x)\n",
        "    output, (h, c) = self.rnn(x_e, hidden)\n",
        "    out = self.out(h)\n",
        "    \n",
        "    y_hat = self.sm(out)\n",
        "  \n",
        "    return y_hat, (h, c)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkwQ3xPjCKC3"
      },
      "source": [
        "**Q1. Answer the questions below:**\n",
        "\n",
        "Read about the Encode-Decoder Architecture here: https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html\n",
        "\n",
        "- In this particular problem, what are the elements of the input sequence (list all possible values)?\n",
        "- In this particular problem, what are the elements of the output sequence (list all possible values)?\n",
        "- In this particular problem, which sequence does the Encoder encode?\n",
        "- Which state of the LSTM does the Encoder use as the encoding of the input sequence (recall that the LSTM calculates a state after each element of the input sequence)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1IdfuAMB4gV"
      },
      "source": [
        "'''\n",
        "The elements of the input sequence include the following four strings: “They”,\n",
        "“are”, “watching”, “.”.\n",
        "\n",
        "The elements of the output sequence include the following three strings:\n",
        "“Ils”, “regardent”, “.”.\n",
        "\n",
        "The encoder encodes the following sequence: “They”, “are”, “watching”, “.”.\n",
        "\n",
        "The state of the LSTM that the Encoder uses as the encoding of the input\n",
        "sequence is the final state that is encoded.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qls313WkH-ZE"
      },
      "source": [
        "import random \n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "       \n",
        "    def forward(self, x, y, teacher_forcing_ratio=0.5):\n",
        "        \n",
        "        # src = [src sent len, batch size]\n",
        "        # trg = [trg sent len, batch size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = y.shape[0]\n",
        "        max_len = y.shape[1]\n",
        "        vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, vocab_size).to(\"cuda\")\n",
        "        \n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden = self.encoder(x)\n",
        "        \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        y_hat = torch.zeros(batch_size, dtype=torch.long).unsqueeze(1)\n",
        "        for t in range(0, max_len):\n",
        " \n",
        "            output, hidden = self.decoder(y_hat, hidden)          \n",
        "            outputs[t] = output\n",
        "    \n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            # the 2 arg is the dimension to reduce; skips batch and\n",
        "            # and pulls max index from each softmax (for each instance)\n",
        "            max_preds = output.max(2)[1] # will be 1 x batch\n",
        "            # tranpose to batch x 1\n",
        "            y_hat = max_preds.transpose(0,1)\n",
        "            if teacher_force:\n",
        "              # then replace predictions with the reference\n",
        "              y_hat = y[:,t].unsqueeze(1)\n",
        "\n",
        "        \n",
        "        # need to flip dims around to be \n",
        "        # (batch x length x vocab)\n",
        "        return outputs.permute(1, 0, 2)\n",
        "  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9tfuwxnLFQM",
        "outputId": "1b58db28-0c65-4413-bc55-9ef1060b17ca"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10,  5,  5,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNrafjnrJSzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd051d5-222b-4944-dd7e-2a5abedf4a0f"
      },
      "source": [
        "embed_dim = 32\n",
        "hidden_dim = 32\n",
        "vocab_size = len(chars)\n",
        "\n",
        "encoder = Encoder(vocab_size, embed_dim, hidden_dim)\n",
        "decoder = Decoder(vocab_size, embed_dim, hidden_dim, vocab_size)\n",
        "\n",
        "s2s = Seq2Seq(encoder, decoder)\n",
        "\n",
        "output = s2s(x[:8], y[:8])\n",
        "\n",
        "output.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DDzVRZ8jeJu",
        "outputId": "1d97c772-575d-4f61-9fd8-a21614d59136"
      },
      "source": [
        "y_target = y[:8]\n",
        "y_target.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4jEga8HqR4s",
        "outputId": "00ac4fa4-6a25-478c-e216-97fb69a7bfc0"
      },
      "source": [
        "y_target[0,:]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10,  5,  5,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5PEzxOpqZGG",
        "outputId": "076d0412-ca3c-4e83-918d-812d8073ae66"
      },
      "source": [
        "output[0,:]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0936, 0.0804, 0.0755, 0.0736, 0.0753, 0.0879, 0.0879, 0.0817, 0.0867,\n",
              "         0.0653, 0.0897, 0.1023],\n",
              "        [0.0886, 0.0768, 0.0955, 0.0800, 0.0747, 0.0745, 0.1063, 0.0833, 0.0847,\n",
              "         0.0631, 0.0877, 0.0848],\n",
              "        [0.1062, 0.0749, 0.0766, 0.0840, 0.0671, 0.1012, 0.0948, 0.0737, 0.0769,\n",
              "         0.0636, 0.0857, 0.0953],\n",
              "        [0.1126, 0.0742, 0.0650, 0.0845, 0.0648, 0.1162, 0.0907, 0.0688, 0.0726,\n",
              "         0.0635, 0.0876, 0.0994]], device='cuda:0', grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16XkEb5WTp-X",
        "outputId": "8631981c-8164-4ce9-cae1-509a7ce49791"
      },
      "source": [
        "y_target.flatten().shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDLURUeJUuF9"
      },
      "source": [
        "C = len(chars)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gji8Xz_RlErb"
      },
      "source": [
        "# need to create (N x C) so collapse first two dims\n",
        "output = output.contiguous().view(-1, C)\n",
        "# this is just going to be (B x 1) -- B being batch size\n",
        "y_target = y_target.flatten().cuda()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdXfuhJlSSM4",
        "outputId": "1b867ae6-96c8-4549-fcb3-6946d7afb854"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCi0wYgPtfRa"
      },
      "source": [
        "from torch import optim\n",
        "optimizer = optim.Adam(s2s.parameters())\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tFQc1ldKYCp",
        "outputId": "74c3743d-e08a-4e22-e11c-9031b6a330f0"
      },
      "source": [
        "loss = criterion(output, y_target)\n",
        "print(loss.item())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.08900000154972076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YdYBVkGiOmu"
      },
      "source": [
        "Now, train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abfRTGIxh-Jg"
      },
      "source": [
        "def train(model, x, y, optimizer, criterion, batch_size=16, epochs=10):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i in range(epochs):\n",
        "      current_idx = 0 \n",
        "   \n",
        "      while (current_idx + batch_size) < x.shape[0]:\n",
        "        optimizer.zero_grad()\n",
        "        batch_x, batch_y = x[current_idx:current_idx+batch_size], y[current_idx:current_idx+batch_size]\n",
        "        \n",
        "        output = model(batch_x, batch_y)\n",
        "        \n",
        "        # flatten\n",
        "        #output_flat = output.contiguous().view(-1, vocab_size)\n",
        "        output_flat = output.contiguous().view(-1, vocab_size)\n",
        "        y_flat = batch_y.view(-1).cuda()\n",
        "       \n",
        "        loss = criterion(output_flat, y_flat)\n",
        "       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        current_idx += batch_size\n",
        "        \n",
        "      print(f\"epoch {i} loss: {epoch_loss:.2f}\")\n",
        "    return model\n",
        "        "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNtacHTItPbp",
        "outputId": "6a064ffb-2842-43d4-e3ba-f619bcf8b9ee"
      },
      "source": [
        "mm = train(s2s, x_train, y_train, optimizer, criterion, epochs=10)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss: -920.94\n",
            "epoch 1 loss: -1870.94\n",
            "epoch 2 loss: -2825.35\n",
            "epoch 3 loss: -3780.35\n",
            "epoch 4 loss: -4736.25\n",
            "epoch 5 loss: -5692.43\n",
            "epoch 6 loss: -6684.26\n",
            "epoch 7 loss: -7764.14\n",
            "epoch 8 loss: -8870.58\n",
            "epoch 9 loss: -9985.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e16QzkRvIcW3"
      },
      "source": [
        "Make predictions for first 10 instances in validation set. We are passing in y_val (which we would not actually have in practice!) just for convienence -- note that we set the teacher forcing ratio to 0, and hence this is not used during decoding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6FwlsJVVW7AS",
        "outputId": "22be016b-1a7b-4f1d-9bf2-bb4d194b4b24"
      },
      "source": [
        "predictions = s2s(x_train[:10], y_train[:10])\n",
        "predictions = predictions.cpu().detach().numpy()\n",
        "print(predictions.shape)\n",
        "ctable.decode(predictions[3,:,:])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 4, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'900 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "e28JNNfLDJWo",
        "outputId": "f5cb6762-b26e-453f-b2fa-8c348624db69"
      },
      "source": [
        "y_train0 = y_train[0].cpu().detach().numpy()\n",
        "ctable.decode(y_train0, calc_argmax=False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'833 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvBvK1UjDGbL"
      },
      "source": [
        "predictions = s2s(x_val[:10], y_val[:10], teacher_forcing_ratio=0)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSbanqmDInye",
        "outputId": "2b93d67e-51f2-44aa-8730-1320a2d7c0d3"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 4, 12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uJBFIdHJRH5"
      },
      "source": [
        "predictions = predictions.cpu().detach().numpy()\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iPIBoudJhlE",
        "outputId": "3584454c-f850-44f0-8aa4-d5a6c5ef8bae"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 4, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "oI_s4L7MJ-Ub",
        "outputId": "ca5e2323-beeb-43fa-cf94-9a0cd1efa179"
      },
      "source": [
        "x_val0 = x_val[1].cpu().detach().numpy()\n",
        "x_val0.shape\n",
        "ctable.decode(x_val0, calc_argmax=False)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'6+759  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "fhds2ag3Ix-J",
        "outputId": "29e03711-49a4-4af6-c25c-6784e7e7eb0c"
      },
      "source": [
        "ctable.decode(predictions[0,:,:])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1111'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztl-lP2nPSpx",
        "outputId": "79dd1045-6a8a-4171-df29-18ef53dab154"
      },
      "source": [
        "predictions[1,0,:]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.0649618e-07, 1.7614746e-10, 7.3748452e-05, 6.4984903e-05,\n",
              "       3.2821061e-09, 5.8934138e-09, 2.2657009e-08, 3.5061223e-08,\n",
              "       4.4543650e-03, 9.9523348e-01, 1.0121244e-04, 7.2022958e-05],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "r0Ia172bK_EZ",
        "outputId": "69547b53-d53b-4907-d6f1-697fa895409a"
      },
      "source": [
        "y_val0 = y_val[1].cpu().detach().numpy()\n",
        "ctable.decode(y_val0, calc_argmax=False)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'765 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iijr_wqcF4-G"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}